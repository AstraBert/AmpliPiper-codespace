# HAPLOTYPES PIPELINE

## TEST DATASETS

```bash 

WD=/media/inter/mkapun/projects

<download raw reads>

bash ${WD}/HAPLOTYPES/shell/pipeline_main.sh \
    -s TestData/samples.csv \
    -p TestData/primers.csv \
    -o TestData/output \
    -q 10 \
    -n 500 \
    -t 100 \
    -f

bash ${WD}/HAPLOTYPES/shell/pipeline_main.sh \
    -s ${WD}/HAPLOTYPES/lepidoptera_test/samples_MK.csv \
    -p ${WD}/HAPLOTYPES/lepidoptera_test/primers.csv \
    -o ${WD}/HAPLOTYPES/lepidoptera_test_output \
    -q 10 \
    -n 500 \
    -t 100 \
    -f

```


## MATERIALS AND METHODS 
### PIPELINE OPTIONS:
Usage: HAPLOTYPES -s, --samples SAMPLES_CSV -p, --primers PRIMERS_CSV -o,--output OUTPUT_FOLDER  [ -q, --quality QUALITY ] 
    [ -n, --nreads NUMBER_OF_READS ] [ -t, --threads NUMBER_OF_THREADS ] [ -f,--force ] [ -b,--blast ]

**REQUIRED:**
   
- `-s | --samples`: Provide the path to a csv file that contains the names (without extension) and the path of the fastq files containing the raw reads that will be analysed; an example is:

```
MERCURIALIS1,/media/User/HAPLOTYPES/vcf_phasing_and_test/SRR17577149.fastq.gz
MERCURIALIS2,/media/User/HAPLOTYPES/vcf_phasing_and_test/SRR17577151.fastq.gz
```
    
- `-p | --primers`: Provide the path to a csv file that contains the IDs, the forward and the reverse sequences of the primers, and the ploidy (1 for haploid, 2 for diploid) of the locus. Find an example in the section dedicated to demultiplexing.
    
- `-o | --output`: Provide the path to the output folder
    
**OPTIONAL:**
    
- `-q | --quality`: Provide an integer that represent the minimum quality which the reads will be filtered for (default is 10)
    
- `-n | --nreads`: Provide the absolute number or the percentage of top quality reads you want the pipeline to take into account for consensus sequences generation and variant calling (default is 10000)
- `-t | --threads`: Number of threads the program will be using (default: 10)
    
- `-f | --force`: Force overwriting if the output folder already exists (default: Cowardly refusing to overwrite)
    
- `-b | --blast`: Enable BLAST search for species identification (default: Not enabled)


Input HAPLOTYPES -h,--help to show the help message
  

### PIPELINE OUTPUT FOLDER STANDARD ARCHITECTURE
The output folder (specified by the user with the ```--output``` option of the pipeline script) folder architecture is as follows:
```
outputfolder
|_data
|  |_raw (contains the copies of the raw reads files)
|  |_demult (contains the demultiplexed files)
|  |_filtered (contains the files with filtered reads)
|
|_results
|  |_consensus_seqs (contains a subfolder for each primer, in which we can find the consensus sequences as generated by AmpliconSorter and variant calling files)
|  |_haplotypes (contains the files resulting from haplotype calling)
|  |_trees (contains the tree files and images)
|
|_shell
   |_demult1 (contains the shellscripts for filtering and demultiplexing)
   |_demult2 (contains the shellscripts for consensus sequences generation, variant and haplotype calling)
```
### STEP 1: READS FILTERING
#### TOOL AND APPLICATION
Nanofilt is implemented in the pipeline to filter the reads according to a minimum quality provided by the user with the ```--quality``` option of the pipeline script. The filtered reads are placed in the ```filtered``` folder as ```samplename-filt.fastq(.gz)```. Samplename is the name of the sample as found in the file provided witht the ```--sample``` option.

### STEP 2: DEMULTIPLEXING 
#### DEVELOPMENT
The idea behind the code is to provide a simple yet powerful implementation of demultiplexing, based on primer identification at the beginning and at the end of the PCR-produced sequences in the raw reads file.

To pursue this aim, ```demultiplex_fastq_readselection.py``` was written and tested.
The code takes, as input, a fastq file with raw basecalled reads and the primers table (in .csv format, mandatorily with ID, FWD, REV and PLOIDY fields) and goes through the following steps, that can also be found in the [commented script](./scripts/demultiplex_fastq_readselection.py):

1. The function ```read_primers_table``` reads primers table: as said before, this table must be provided as a csv file whose separator must be comma and whose fields must be named and ordered as follows: ID,FWD,REV,PLOIDY
   - ID: contains the demultiplexing unit to which the primers are referred
   - FWD and REV: idicate respectively the forward and reverse primer sequences
   
   Here's an example of how the csv should look:
   ```
   ID,FWD,REV,PLOIDY
   RH1,TTTCTGTTGGTGCTGATATTGCGCATACTCACTCATGGCTGC,ACTTGCCTGTCGCTCTATCTTCCTCTGATCCCTGGTTGCTGA,2
   RH2,TTTCTGTTGGTGCTGATATTGCGTTCAGAGCCAGATCAGAT,ACTTGCCTGTCGCTCTATCTTCATCTCGTGATCCAATCTCAA,2
   LWS,TTTCTGTTGGTGCTGATATTGCTCTTCTGCTACATTTTCGTG,ACTTGCCTGTCGCTCTATCTTCTAGGTTTCGTGGGTAATGTT,2
   SWS1,TTTCTGTTGGTGCTGATATTGCAGACCTGAATGTGACTTTTA,ACTTGCCTGTCGCTCTATCTTCGAAGTCCTTTGCCATCTTG,2
   SWS2,TTTCTGTTGGTGCTGATATTGCAGAAGGAGGTCACCAAGAT,ACTTGCCTGTCGCTCTATCTTCGCTGCTGAAGTCTAAATTTGA,2
   ```
2. The function ```load_data``` reads provided to-demultplex file, only if it is in fasta or fastq format, and returns a list with all the sequences
3. The function ```demultiplex``` actually does the demultiplexing job. It takes every sequence from the list previously extracted and aligns the first len(primer)*2 bp against every primer: the forward primer comes first and, if a significant alignment is produced, the last len(primer)*2 bp are aligned against the reverse complemented reverse primer. How is an alignment labelled as significant? For the alignment, the ```edlib``` library is used: it provides the code with a faster implementation of edit distance calculation (an already optimized version of Levenshtein's distance); to make it even faster, the program is instructed to mark as "non-significant" every alignment that produces an edit distance which is bigger than len(primer). The output of this function is a dictionary that contains the IDs as keys and the lists of demultiplexed reads referred to them as values
4. The function ```write_demult_files``` writes the temporary demultiplexed reads in separate files: the demultiplexed files will be named after the IDs, and they will be in fastq format. To understand the naming conventions, let's consider these two examples. They will be put in the folder provided with the `output` argument.
5. The function ```select_perc_reads``` writes the definitively demultiplexed files, that will contain a selected number of reads, that will represent the percentage (or absolute number) of raw sequences, as provided by the user with the ```--reads_percentage``` option, that show the highest quality scores.
Example command to run the script:

```bash
python3 /home/user/scripts/demultiplex_fastq_readselection.py -i /home/user/project/reads/individual1.fastq -p /home/user/project/data/primers.csv -o /home/user/project/reads/demult  -rp 0.8
```

#### TESTING

The demultiplexer was tested on an artificially generated file with Coleoptera-related sequences: primers were designed on the first and the last 25 bp for each of the three genes taken into account for the demultiplexing. Demultiplexing took 03.473135 s and yielded a 0% contamination rate (checked by BLASTing demultiplexed files against references). Generation of consensus sequences with AmpliconSorter produced 1 cons/demultiplexed file and these consensus sequences had high similarity (99-100%) and high query coverage (99-100%) when blasted against reference sequences.

The demultiplexer was tested on real data from _Phoxinus phoxinus_: primers were provided and did not need any further modification. Demultiplexing took 14.897929 s and yielded 0% contamination rate (checked by BLASTing demultiplexed files against references). Generation of consensus sequences with AmpliconSorter produced 1 or 2 cons/demultiplexed file (when 2, it was because of some short, <400 bp, sequences that were probably PCR byproducts) and these consensus sequences had high similarity (99-100%) and high query coverage (99-100%) when blasted against reference sequences.

The demultiplexer was tested also with another setting on _Phoxinus phoxinus_: the first 100bp were taken into account for the raw sequences, regardless of the primer length. This setting generated a high contamination rate (sometimes more than 60%), probably due to the limitation of the alignment algorithm. Based on that, a decision was made to continue with the primer length approach. 

#### APPLICATION IN THE PIPELINE
The input files from the samples csv are demultiplexed and their demultiplexed files, named with the IDs of the primers, are placed within subfolders (that are named as the samplename from the samples csv) in the ```demultiplexed``` directory.

### STEP 2: CONSENSUS SEQUENCES GENERATION
#### CHOSEN TOOL AND REASONS FOR CHOICE
Amplicon_Sorter (Vierstraete et al., 2022) was chosen as consensus sequences generator. It was compared with Decona (Doorenspleet et al., 2021) and NGSpeciesID (Sahlin et al., 2021), and it was judged as more suitable to the pipeline needs for the following reasons:

- It is CPU-time efficient and has a relatively low real-time consumption
- It produces accurate results, that are higly similar to reference sequences
- It outputs a number of consensus sequences that is generally close to the expected one
- It exploits a simple, straightforward and yet powerful command line
- It is easy to install and requires no maintenance or modifications, being a solidly built piece of code

####  APPLICATION IN THE PIPELINE
Amplicon_sorter is run in loop on every demultiplexed file: the results are placed in a folder named "results/consensus_seqs" and each sample from samples csv has its own sub-folder, that contains other subfolders named as the primers IDs, in which you will find the consensus sequences.

### STEP 3: CONSENSUS SEQUENCE SELECTION AND MAPPING
#### 3.1: CONSENSUS SEQUENCE SELECTION
Before mapping can start, checking whether there is only one consensus sequence or there are actually more is key to make this third step and the following downstream analysis more simple and less convolute. The script choose_consseq.py is responsible for this checkpoint: it takes, as argument, the AmpliconSorter output file containing the concatenated consensus sequences. The python code first reads the mentioned file, passing it over if it has only one cons seq, while returning a dictionary with the headers as keys and the reads as values in case there are two or more cons seqs: in this latter situation, the format of AmpliconSorter headers reports also the number of supporting reads for each cons seq, so the program recognizes this number, sorts the sequences according to it and spits out the one with the highest support. The output of choose_consseq.py is a fasta file named _selectedconsensus.fasta_, which will be produced in every cases and will be placed in the same folder as the input file used to generate it. 
#### 3.2: MAPPING 
**From this step on, all that we say is carried on only on diploid samples.**
Mapping is performed by minimap2 (Li, 2018) and is performed in loop on every consensus sequence from Amplicon_Sorter (reference), against which are mapped raw reads from the demultiplexed file that generated it. At first, minimap2 is used to index the consensus, producing a .mmi file, which is in turn used as reference for the actual mapping: as a result, a .sam file is obtained. This last file is then converted into a .bam, sorted and indexed, on the fly, all at once, by samtools-sort. Samtools, in its faidx mode, is also used to index the consensus sequence for later bcftools application.

### STEP 4: VARIANT CALLING AND PHASING
#### TOOLS AND APPLICATION
Variant calling is performed with the help of NanoCaller (Ahsan et al., 2021), a machine learning program to specifically call variants on Nanopore reads. Variant phasing is performed within NanoCaller with WhatsHap (Schrinner et al., 2020; Martin et al., preprint). The results are stored within the same folder fo the consensus sequences, but in a subfolder named exactly as the parent one.

### STEP 5: HAPLOTYPE WRITING 
#### TOOLS AND APPLICATION
Bcftools-consensus is applied to write haplotypes from phased VCF files resulting from NanoCaller analyses: the two haplotypes are reported in the "results/haplotypes" folder.

### STEP 6: ALIGNMENT AND PHYLOGENETIC ANALYSIS
#### TOOLS AND APPLICATION
mafft () is used to perform alignment of concatenated haplotypized consensus sequences (concatenated from all the individuals, for each primer), that are then piped into IQtree (), that generates a multiple-individual tree for each locus. 
Astral () is used to concatenate all the primer sequences from an individual and to align all the so-concatenated individuals, in order to infer a phylogenetic tree that accounts for multiple-individual-multiple-locus genetic variation.

